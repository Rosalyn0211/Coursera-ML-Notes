# 逻辑回归

### 分类问题
在分类问题中，你要预测的变量 $y$ 是离散的值，我们将利用逻辑回归 (Logistic Regression) 算法。

### 假说表示
 逻辑回归模型的假设是： $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$   
 其中： $X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function），公式为：  
 $g\left( z \right)=\frac{1}{1+{{e}^{-z}}}$。
 
 ### 判定边界
在逻辑回归中，我们预测：  
当${h_\theta}\left( x \right)>=0.5$时，预测 $y=1$。  
当${h_\theta}\left( x \right)<0.5$时，预测 $y=0$ 。  

![image](image/逻辑回归.png)  
现在假设我们有一个模型：  
![image](image/边界决策.png)  
并且参数$\theta$ 是向量[-3 1 1]。 则当$-3+{x_1}+{x_2} \geq 0$，即${x_1}+{x_2} \geq 3$时，模型将预测 $y=1$。 我们可以绘制直线${x_1}+{x_2} = 3$，,将预测为1的区域和预测为 0的区域分隔开。  
假使我们的数据呈现这样的分布情况，怎样的模型才能适合呢？  
![image](image/边界决策2.jpg)  

因为需要用曲线才能分隔 $y=0$ 的区域和 $y=1$ 的区域，我们需要二次方特征：${h_\theta}\left( x \right)=g\left( {\theta_0}+{\theta_1}{x_1}+{\theta_{2}}{x_{2}}+{\theta_{3}}x_{1}^{2}+{\theta_{4}}x_{2}^{2} \right)$是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为1的圆形。

我们可以用非常复杂的模型来适应非常复杂形状的判定边界。  

### 代价函数
对于线性回归模型，我们定义的代价函数是所有模型误差的平方和。理论上来说，我们也可以对逻辑回归模型沿用这个定义，但是问题在于，当我们将${h_\theta}\left( x \right)=\frac{1}{1+{e^{-\theta^{T}x}}}$带入到这样定义了的代价函数中时，我们得到的代价函数将是一个非凸函数（non-convexfunction）。  
![image](image/代价函数.jpg)  
这意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。  
线性回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{1}{2}{{\left( {h_\theta}\left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2}}}$ 。 我们重新定义逻辑回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{{Cost}\left( {h_\theta}\left( {x}^{\left( i \right)} \right),{y}^{\left( i \right)} \right)}$，其中  
![image](image/代价函数2.png)    
在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：

Repeat { $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$ (simultaneously update all ) }

求导后得到：

Repeat { $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{{\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)}}\mathop{x}_{j}^{(i)}$ (simultaneously update all ) }  

一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有：共轭梯度（Conjugate Gradient），局部优化法(Broyden fletcher goldfarb shann,BFGS)和有限内存局部优化法(LBFGS)。  

### 多类别分类
对于一个多类分类问题，我们的数据集或许看起来像这样：  

![image](image/一对多分类.png)    
我们下面要做的就是使用一个训练集，将其分成3个二元分类问题。
我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的"伪"训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。  

![image](image/一对多分类2.png)    
为了能实现这样的转变，我们将多个类中的一个类标记为正向类（$y=1$），然后将其他所有类都标记为负向类，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地第我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。 最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3....k \right)$

最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，即$\mathop{\max}\limits_i,h_\theta^{\left( i \right)}\left( x \right)$。

### python实现
 
 逻辑回归用于处理分类问题，其输出结果是离散的。

##### 如何工作？
逻辑回归使用基础逻辑函数通过估算概率来预测因变量的标签与自变量的关系。
这些概率值必须转换为二进制，以便实际中进行预测。因此，需要使用逻辑函数sigmoid函数。然后使用阈值分类器将（0,1）范围的值转化成0和1的值来表示结果。

##### sigmoid函数
S形曲线，将任意值映射为（0,1）范围内的值。
<br>

##### 逻辑回归VS线性回归
 - **不同点：**
逻辑回归处理的是分类问题，线性回归处理的是回归问题，这是两者最本质的区别。
逻辑回归的因变量是离散的，线性回归的因变量是连续的，这是两者最大的区别。
- **相同点：**
两者都使用了极大似然估计来对训练样本进行建模。其中，线性回归使用最小二乘法实际上就是自变量与超参数确定的，因变量服从正态分布的假设下，使用极大似然估计的一个化简。
二者在求解超参数的过程中都使用了梯度下降的方法。

##### python 实现
数据说明：该数据集包含了社交网络中用户的信息。这些信息涉及用户ID,性别,年龄以及预估薪资。一家汽车公司刚刚推出了他们新型的豪华SUV，我们尝试预测哪些用户会购买这种全新SUV，并且在最后一列用来表示用户是否购买。我们将建立一种模型来预测用户是否购买这种SUV，该模型基于两个变量，分别是年龄和预计薪资。因此我们的特征矩阵将是这两列。我们尝试寻找用户年龄与预估薪资之间的某种相关性，以及他是否购买SUV的决定。
[数据从这里获取](https://github.com/Avik-Jain/100-Days-Of-ML-Code/blob/master/datasets/Social_Network_Ads.csv)

Step 1 数据预处理
```
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset = pd.read_csv(r'')
X = dataset.iloc[ : , [2,3]].values
Y = dataset.iloc[ : ,4].values

X_train, Y_train, X_test, Y_test = train_test_split(X,Y,test_size=0.25, random_state=0)

sc = StandardScaler
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
```
Step 2 逻辑回归模型

该项工作的库是一个线性模型库，因为逻辑回归是一个线性分类器，这意味着我们在二维空间中，我们两类用户（购买和不购买）将被一条直线分割。然后导入逻辑回归类。下一步我们将创建该类的对象，它将作为我们训练集的分类器。

```
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)
```

Step 3 预测

```
y_pred = classifier.predict(X_test)
```

Step 4 预测评估

生成混淆矩阵
```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```
可视化

```
from matplotlib.colors import ListedColormap
X_set,y_set=X_train,y_train
X1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),
                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(),X1.max())
plt.ylim(X2.min(),X2.max())
for i,j in enumerate(np. unique(y_set)):
    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],
                c = ListedColormap(('red', 'green'))(i), label=j)

plt. title(' LOGISTIC(Training set)')
plt. xlabel(' Age')
plt. ylabel(' Estimated Salary')
plt. legend()
plt. show()

X_set,y_set=X_test,y_test
X1,X2=np. meshgrid(np. arange(start=X_set[:,0].min()-1, stop=X_set[:, 0].max()+1, step=0.01),
                   np. arange(start=X_set[:,1].min()-1, stop=X_set[:,1].max()+1, step=0.01))

plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),
             alpha = 0.75, cmap = ListedColormap(('red', 'green')))
plt.xlim(X1.min(),X1.max())
plt.ylim(X2.min(),X2.max())
for i,j in enumerate(np. unique(y_set)):
    plt.scatter(X_set[y_set==j,0],X_set[y_set==j,1],
                c = ListedColormap(('red', 'green'))(i), label=j)

plt. title(' LOGISTIC(Test set)')
plt. xlabel(' Age')
plt. ylabel(' Estimated Salary')
plt. legend()
plt. show()

```

注释  

 - np.meshgrid()
np.meshgrid从坐标向量返回坐标矩阵。
在这里meshgrid在二维平面中将每一个x和每一个y分别对应起来，编织成栅格。

 - plt.contourf()
 使用函数plt.contourf把颜色加进去，位置参数分别为：X, Y, f(X,Y)。透明度0.75。
  - ravel()
  numpy中的ravel()、flatten()、squeeze()都有将多维数组转换为一维数组的功能.
  - T
 对矩阵进行转置
  - plt.xlim/plt.ylim
  使用 plt.xlim 设置x坐标轴范围； 使用 plt.ylim 设置y坐标轴范围。
 - np.unique()
对于一维数组或者列表，unique函数去除其中重复的元素，并按元素由大到小返回一个新的无元素重复的元组或者列表。
 - X_set[y_set==j,0]
 用一个矩阵索引另一个矩阵。

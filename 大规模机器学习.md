# 大规模机器学习(Large Scale Machine Learning)
### 大型数据集的学习
首先检查一个大规模的训练集是否真的必要，也许只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。
### 随机梯度下降法
如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。

在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：

​ $$cost\left( \theta, \left( {x}^{(i)} , {y}^{(i)} \right) \right) = \frac{1}{2}\left( {h}_{\theta}\left({x}^{(i)}\right)-{y}^{{(i)}} \right)^{2}$$

随机梯度下降算法为：首先对训练集随机“洗牌”，然后： Repeat (usually anywhere between1-10){

for $i = 1:m${

​ $\theta:={\theta}{j}-\alpha\left( {h}{\theta}\left({x}^{(i)}\right)-{y}^{(i)} \right){{x}_{j}}^{(i)}$

​ (for $j=0:n$)

​ } }

随机梯度下降算法在每一次计算之后便更新参数 ${{\theta }}$ ，而不需要将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。
### 小批量梯度下降
小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数 ${{\theta }}$ 。 Repeat {

for $i = 1:m${

​ $\theta:={\theta}{j}-\alpha\frac{1}{b}\sum\limits{k=i}^{i+b-1}\left( {h}{\theta}\left({x}^{(k)}\right)-{y}^{(k)} \right){{x}{j}}^{(k)}$

​ (for $j=0:n$)

​ $ i +=10 $

​ } }

通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。
### 随机梯度下降收敛
在随机梯度下降中，我们在每一次更新 ${{\theta }}$ 之前都计算一次代价，然后每$x$次迭代后，求出这$x$次对训练实例计算代价的平均值，然后绘制这些平均值与$x$次迭代的次数之间的函数图表。


当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加$α$来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。

如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率$α$。

我们也可以令学习率随着迭代次数的增加而减小，例如令：

​ $$\alpha = \frac{const1}{iterationNumber + const2}$$


